# some code is taken from tutorials of the FAISS library. Taken from https://github.com/facebookresearch/faiss
import argparse
from typing import Tuple

import faiss  # make faiss available
import torch
import os
import tqdm
import numpy as np
import time
import logging
from constants import (
    INDEX_FILE,
    TOKEN_ID_FILE,
    RAW_FEATURE_VALUE_SUFFIX,
    RAW_FEATURE_KEY_SUFFIX,
)

np.random.seed(1234)

# set up logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO,
)


class DataStore:
    """
    This class represents a datastore. It can be trained from raw features, saved and loaded from disk.
    During inference time, it can search given a query and return the normalized score for each token.
    """

    def __init__(self):
        """
        Set the necessary attributes. The number follow the original paper.
        """
        co = faiss.GpuClonerOptions()
        co.useFloat16 = True  # to avoid GPU memory issue
        resources = faiss.StandardGpuResources()
        d = 1024  # dimension of keys
        n_centroids = 4096  # number of clustering centroids
        code_size = 64
        quantizer = faiss.IndexFlatL2(d)
        index = faiss.IndexIVFPQ(quantizer, d, n_centroids, code_size, 8)
        index = faiss.index_cpu_to_gpu(resources, 0, index, co)
        index.nprobe = 32
        self.vocab_size = -1  # to be set later
        self.index = index
        self.T = 10  # temperature as described in the paper
        self.max_num_training_keys = 1000000

    def load(self, saved_dir: str) -> None:
        """
        Load the pretrained FAISS index from a directory with the necessary components. The directory should include:
        - index.trained : the trained index
        - token_ids.pt : the token ids sorted by their index id in FAISS.
        :param saved_dir: The directory containing the trained index.
        :return: None. The attributes of this Datastore instance will be set.
        """
        logger.info(
            f"Started loading trained index and token ids lookup from {saved_dir}"
        )
        self.index = faiss.read_index(os.path.join(saved_dir, INDEX_FILE))
        self.token_lookup_tensor = torch.tensor(
            torch.load(os.path.join(saved_dir, TOKEN_ID_FILE))
        )
        logger.info(f"Finished loading trained index and token ids from {saved_dir}")

    def train_index(self, key_store) -> None:
        """
        Training the FAISS index. We will perform random sampling on the keys.
        :param key_store: a numpy array with shape (num_keys, dim_keys), each row is a key
        :return: None. The index attribute will be updated after training.
        """
        logger.info(f"Start training the index, this might take a long time.")
        random_indices = np.random.choice(
            np.arange(len(key_store)),
            size=[min(1000000, len(key_store))],
            replace=False,
        )
        start = time.time()
        self.index.train(key_store[random_indices])
        logger.info(
            f"Finished training the index. It took {(time.time() - start)} seconds."
        )
        self.index = faiss.index_gpu_to_cpu(self.index)  # put back to CPU

    def read_feature_files(self, feature_dir, percentage=100) -> Tuple:
        """
        Read the raw features generated by generate_raw_feature.py, and stack them into on single tensor.
        :param feature_dir: The directory containing the raw features.
        :param percentage: The percentage of files to read from (mainly for testing purpose).
        :return:
        key_store: a numpy array of shape (num_keys, dim_keys), each row is a key
        token_id_store: a numpy array of shape (num_keys, 1), each row represents the value (target token) to the key.
        """
        value_files = list(
            filter(
                lambda x: x.endswith(RAW_FEATURE_VALUE_SUFFIX), os.listdir(feature_dir)
            )
        )
        value_files = value_files[: int(len(value_files) * (percentage / 100.0))]
        key_store = []
        token_id_store = []
        start_time = time.time()
        for file_name in tqdm.tqdm(
            value_files, total=len(value_files), desc="Loading feature files"
        ):
            file_id = file_name.split(RAW_FEATURE_VALUE_SUFFIX)[0]
            key_path = os.path.join(feature_dir, str(file_id) + RAW_FEATURE_KEY_SUFFIX)
            value_path = os.path.join(
                feature_dir, str(file_id) + RAW_FEATURE_VALUE_SUFFIX
            )
            try:
                curr_keys = torch.load(key_path)
                curr_token_ids = torch.load(value_path)
            except Exception as e:
                logger.error(f"Failed to load {key_path} or {value_path}.")
                raise IOError(e)
            key_store += (
                curr_keys.cpu()
            )  # ensure that it is on CPU, as numpy doesn't support GPU
            token_id_store += curr_token_ids.cpu()
        key_store = np.stack(key_store)
        token_id_store = np.stack(token_id_store)
        logger.info(
            f"Successfully loaded {len(key_store)} keys and values, used {time.time() - start_time} seconds"
        )
        return key_store, token_id_store

    def read_features_and_train(
        self, feature_dir: str, output_dir: str, percentage: int = 100
    ) -> None:
        """
        Read features and train the index. The result will be saved.
        :param feature_dir: The directory containing the raw features from generate_raw_features.py
        :param output_dir: The output directory to save the trained index and index-to-token mapping.
        :param percentage: The percentage of the all features to perform training
        :return: None. The trained index will be saved to output_dir.
        """
        key_store, token_id_store = self.read_feature_files(
            feature_dir=feature_dir, percentage=percentage
        )
        self.token_lookup_tensor = torch.tensor(token_id_store)
        self.train_index(key_store)
        self.add_keys(key_store)
        self.save(output_dir)
        return

    def add_keys(self, keys_to_add: np.ndarray) -> None:
        """
        Add the keys to the trained index.
        :param keys_to_add: a numpy array of shape (num_keys, keys_dim)
        :return: The index will be updated with the input keys.
        """
        logger.info("Start adding keys to the index.")
        start_time = time.time()
        self.index.add(keys_to_add)  # add vectors to the index
        logger.info(
            f"Finished adding keys to the index. It took {time.time() - start_time} seconds"
        )

    def save(self, output_dir: str) -> None:
        """
        Save the index and the index-to-token mapping in the output_dir.
        :param output_dir: The directory to save the results.
        :return: None. Results will be saved to output_dir.
        """
        try:
            # write the trained index
            faiss.write_index(self.index, os.path.join(output_dir, INDEX_FILE))
        except Exception as e:
            logger.error(f"Encountered error when writing FAISS index to {output_dir}")
            raise IOError(e)

        try:
            # save the index for token_ids
            torch.save(
                self.token_lookup_tensor, os.path.join(output_dir, TOKEN_ID_FILE)
            )
        except Exception as e:
            logger.error(f"Encountered error when saving torch tensor to {output_dir}")
            raise IOError(e)
        logger.info(
            f"Successfully saved the trained index ({INDEX_FILE}, and {TOKEN_ID_FILE}) to {output_dir}"
        )

    def set_vocab_size(self, vocab_size: int) -> None:
        """
        Set the vocabulary size of the datastore. This will be used when generating score tensors for each token.
        :param vocab_size: size of the vocab of the language model.
        :return: None. the attribute will be set
        """
        self.vocab_size = vocab_size

    def search_k(self, query: torch.tensor, k: int) -> None:
        """
        Search for the top K nearest neighbors, along with the distance.
        :param k: top k
        :param query: should have shape (num_queries, dim_keys).
        :return: scores: should have shape (num_queries, vocab_size), contains scores for each token for each entry
        """
        assert (
            self.vocab_size >= 1
        ), "Please set the vocab size first (using set_vocab_size method) before the search!"
        D, I = self.index.search(
            query, k
        )  # D, I will have shape (num_queries, k), containing the distance and the index
        actual_token_ids = self.token_lookup_tensor[torch.tensor(I)]  # (num_queries, k)
        scores = torch.zeros((query.shape[0], self.vocab_size))
        distance_scores = torch.softmax(
            -torch.tensor(D) / self.T, dim=-1
        )  # softmax of the distance
        scores = scores.scatter(
            1, actual_token_ids, distance_scores, reduce="add"
        )  # will assign the scores to indices and aggregate for each token
        return scores


def parse_args():
    parser = argparse.ArgumentParser(
        description="Generate raw feature tensors for building the datastore"
    )
    parser.add_argument(
        "--feature_dir",
        type=str,
        required=True,
        help="the directory of the generated raw features",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="the directory to save the trained index files",
    )
    parser.add_argument(
        "--sample_percentage",
        type=int,
        default=100,  # by default, use all available data
        help="The percentage to use for training",
    )
    args = parser.parse_args()

    return args


def read_and_train():
    """
    Read the raw features and train the index.
    :return:
    """
    if not (torch.cuda.is_available()):
        logger.warning(
            "No GPU detected in the environment. Not training on GPU can be very slow"
        )

    args = parse_args()
    datastore = DataStore()
    datastore.read_features_and_train(
        feature_dir=args.feature_dir,
        output_dir=args.output_dir,
        percentage=args.sample_percentage,
    )


if __name__ == "__main__":
    read_and_train()

    # The following code is for testing purpose. You can ignore it during the actual run time.
    #
    # d = 64  # dimension
    # nb = 100000  # database size
    # nq = 10000  # nb of queries
    # np.random.seed(1234)  # make reproducible
    # xb = np.random.random((nb, d)).astype('float32')
    # xb[:, 0] += np.arange(nb) / 1000.
    # xq = np.random.random((nq, d)).astype('float32')
    # xq[:, 0] += np.arange(nq) / 1000.
    #
    # print(xb.shape)
    # print(xq.shape)
    # import faiss  # make faiss available
    #
    # index = faiss.IndexFlatL2(d)  # build the index
    # print(index.is_trained)
    # index.add(xb)  # add vectors to the index
    # print(index.ntotal)
    # k = 4  # we want to see 4 nearest neighbors
    # D, I = index.search(xb[:5], k)  # sanity check
    # print(I)
    # print(D)
    # D, I = index.search(xq, k)  # actual search
    # print(I[:5])  # neighbors of the 5 first queries
    # print(I[-5:])  # neighbors of the 5 last queries
    #

    # a = {1:2, 3:4, 5:6}
    # for key, value in zip(*a.items()):
    #     print(key, value)
    # print("number of GPUs", faiss.get_num_gpu())
    # if True:
    #     # if this fails, it means that the GPU version was not comp
    #     assert faiss.StandardGpuResources, \
    #         "FAISS was not compiled with GPU support, or loading _swigfaiss_gpu.so failed"
    #     res = faiss.StandardGpuResources()
    #     dev_no = 0
    # print("No problem in initializing gpu")

    # print('Starting to initialise')
    # datastore = DataStore(config = {"saved_dir": "./saved_gen", "vocab_size":40032})
    # datastore.read_features_and_train(feature_dir = "./saved_gen", output_dir = "./datastore_1", percentage=1)
    # print("Finished initialise")
    # a = torch.tensor([[1,2,3],[0,1,2]])
    # b = torch.tensor([[0.2,0.4,0.5],[2,4,5]])
    # c = torch.zeros(2,5)
    #
    # print(c[a].shape)
    # # c[a]=b
    # print(c)
    # print(a.shape)
    # import torch
    #
    # input = torch.randn(2, 4)
    # print(input)
    # output = torch.zeros(2, 5)
    # index = torch.tensor([[3, 1, 0, 0], [1, 2, 0, 3]])
    # output = output.scatter(1, index, input, reduce="add")
    # print(output)
    #
    # a = torch.tensor(torch.arange(10000)) + 1
    #
    # print(a.shape)
    # index = torch.tensor([[3, 1, 0, 0], [1, 2, 0, 3]])
    # print(a[index].shape)
    # print(a[index])
    # T = 10
    # D = torch.tensor([[3,3,2,1], [3,4,5,1]])
    # D = D/T
    # D = torch.softmax(D, dim=-1)
    # print(D)
    #
    # query = torch.randn((7, 1024)).cpu().numpy()
    # scores = datastore.search_k(query, k=13)
    # print(scores.shape)

    # import numpy as np
    #
    # d = 64  # dimension
    # nb = 100000  # database size
    # nq = 10000  # nb of queries
    # np.random.seed(1234)  # make reproducible
    # xb = np.random.random((nb, d)).astype('float32')
    # xb[:, 0] += np.arange(nb) / 1000.
    # xq = np.random.random((nq, d)).astype('float32')
    # xq[:, 0] += np.arange(nq) / 1000.
    #
    # import faiss
    #
    # nlist = 100
    # m = 8
    # k = 4
    # quantizer = faiss.IndexFlatL2(d)  # this remains the same
    # index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8)
    # # 8 specifies that each sub-vector is encoded as 8 bits
    # print('Starting training')
    # index.train(xb)
    # print("End training")
    # index.add(xb)
    # D, I = index.search(xb[:5], k)  # sanity check
    # print(I)
    # print(D)
    # index.nprobe = 10  # make comparable with experiment above
    # D, I = index.search(xq, k)  # search
    # print(I[-5:])
